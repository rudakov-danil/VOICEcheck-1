"""
Tests for LLM Analyzer service.
"""

import pytest
import asyncio
from unittest.mock import Mock, patch
from datetime import datetime

from app.llm_analyzer import LLMAnalyzer, DialogAnalysis
from app.models import Segment


@pytest.fixture
def sample_segments():
    """Sample segments for testing."""
    return [
        Segment(
            start=0.0,
            end=10.0,
            text="Ð—Ð´Ñ€Ð°Ð²ÑÑ‚Ð²ÑƒÐ¹Ñ‚Ðµ, ÑÑ‚Ð¾ ÐÐ»ÐµÐºÑÐµÐ¹ Ð¸Ð· ÐºÐ¾Ð¼Ð¿Ð°Ð½Ð¸Ð¸ TechSolutions",
            speaker="SPEAKER_00"
        ),
        Segment(
            start=10.0,
            end=20.0,
            text="Ð—Ð´Ñ€Ð°Ð²ÑÑ‚Ð²ÑƒÐ¹Ñ‚Ðµ, Ð¼ÐµÐ½Ñ Ð·Ð¾Ð²ÑƒÑ‚ ÐœÐ°Ñ€Ð¸Ñ. Ð§Ñ‚Ð¾ Ñƒ Ð²Ð°Ñ Ð·Ð° Ð¿Ñ€ÐµÐ´Ð»Ð¾Ð¶ÐµÐ½Ð¸Ðµ?",
            speaker="SPEAKER_01"
        ),
        Segment(
            start=20.0,
            end=30.0,
            text="ÐœÑ‹ Ð¿Ñ€ÐµÐ´Ð»Ð°Ð³Ð°ÐµÐ¼ ÑÐ¾Ð²Ñ€ÐµÐ¼ÐµÐ½Ð½Ð¾Ðµ Ñ€ÐµÑˆÐµÐ½Ð¸Ðµ Ð´Ð»Ñ Ð°Ð²Ñ‚Ð¾Ð¼Ð°Ñ‚Ð¸Ð·Ð°Ñ†Ð¸Ð¸ Ð±Ð¸Ð·Ð½ÐµÑÐ°",
            speaker="SPEAKER_00"
        )
    ]


@pytest.fixture
def llm_analyzer():
    """LLM analyzer instance with mocked API."""
    with patch('app.llm_analyzer.LLMAnalyzer._get_api_key') as mock_key:
        mock_key.return_value = "test_key"
        analyzer = LLMAnalyzer()
        analyzer._call_llm_api = Mock()
        return analyzer


@pytest.mark.asyncio
async def test_analyze_dialog_success(llm_analyzer, sample_segments):
    """Test successful dialogue analysis."""
    # Mock LLM response
    mock_response = """
    {
        "scores": {
            "greeting": 8.0,
            "needs_discovery": 7.0,
            "presentation": 6.0,
            "objection_handling": 9.0,
            "closing": 5.0,
            "active_listening": 8.0,
            "empathy": 7.0,
            "overall": 7.3
        },
        "status": "in_progress",
        "key_moments": [
            {
                "type": "interest",
                "time": 10.0,
                "text": "Ð§Ñ‚Ð¾ Ñƒ Ð²Ð°Ñ Ð·Ð° Ð¿Ñ€ÐµÐ´Ð»Ð¾Ð¶ÐµÐ½Ð¸Ðµ?"
            }
        ],
        "recommendations": [
            {
                "text": "Ð£Ð»ÑƒÑ‡ÑˆÐ¸Ñ‚ÑŒ Ð·Ð°ÐºÑ€Ñ‹Ð²Ð°ÑŽÑ‰Ð¸Ðµ Ð²Ð¾Ð¿Ñ€Ð¾ÑÑ‹",
                "time_range": [25, 30]
            }
        ],
        "speaking_time": {
            "sales": 20,
            "customer": 10
        }
    }
    """

    llm_analyzer._call_llm_api.return_value = mock_response

    # Perform analysis
    result = await llm_analyzer.analyze_dialog(sample_segments)

    # Verify result
    assert isinstance(result, DialogAnalysis)
    assert result.scores["greeting"] == 8.0
    assert result.status == "in_progress"
    assert len(result.key_moments) == 1
    assert len(result.recommendations) == 1
    assert result.speaking_time["sales"] == 20
    assert result.speaking_time["customer"] == 10


@pytest.mark.asyncio
async def test_analyze_dialog_cache(llm_analyzer, sample_segments):
    """Test that analysis results are cached."""
    # Mock LLM response
    mock_response = """
    {
        "scores": {"greeting": 8.0, "needs_discovery": 7.0, "presentation": 6.0,
                  "objection_handling": 9.0, "closing": 5.0, "active_listening": 8.0,
                  "empathy": 7.0, "overall": 7.3},
        "status": "in_progress",
        "key_moments": [],
        "recommendations": [],
        "speaking_time": {"sales": 20, "customer": 10}
    }
    """

    llm_analyzer._call_llm_api.return_value = mock_response

    # First analysis should call API
    await llm_analyzer.analyze_dialog(sample_segments)
    assert llm_analyzer._call_llm_api.call_count == 1

    # Second analysis should use cache
    await llm_analyzer.analyze_dialog(sample_segments)
    assert llm_analyzer._call_llm_api.call_count == 1  # No additional call


def test_get_analysis_summary(llm_analyzer, sample_segments):
    """Test analysis summary generation."""
    # Create mock analysis
    analysis = DialogAnalysis(
        scores={"greeting": 5.0, "needs_discovery": 7.0, "presentation": 6.0,
               "objection_handling": 4.0, "closing": 5.0, "active_listening": 8.0,
               "empathy": 6.0, "overall": 5.9},
        status="in_progress",
        key_moments=[],
        recommendations=[],
        speaking_time={"sales": 100, "customer": 150},
        confidence=0.85,
        reasoning="Test analysis"
    )

    summary = llm_analyzer.get_analysis_summary(analysis)

    # Verify summary structure
    assert "scores" in summary
    assert "status_display" in summary
    assert "status_display" in summary
    assert "key_moments_count" in summary
    assert "recommendations" in summary
    assert "speaking_time_percentages" in summary
    assert "improvement_areas" in summary

    # Verify status display
    assert summary["status_display"] == "ðŸ”„ Ð’ Ñ€Ð°Ð±Ð¾Ñ‚Ðµ"

    # Verify improvement areas
    assert "Ð Ð°Ð±Ð¾Ñ‚Ð° Ñ Ð²Ð¾Ð·Ñ€Ð°Ð¶ÐµÐ½Ð¸ÑÐ¼Ð¸" in summary["improvement_areas"]


def test_get_improvement_areas(llm_analyzer):
    """Test improvement areas identification."""
    low_scores = {
        "greeting": 4.0,
        "needs_discovery": 7.0,
        "presentation": 5.0,
        "objection_handling": 3.0,
        "closing": 4.0,
        "active_listening": 8.0,
        "empathy": 6.0
    }

    high_scores = {
        "greeting": 9.0,
        "needs_discovery": 8.0,
        "presentation": 9.0,
        "objection_handling": 8.0,
        "closing": 9.0,
        "active_listening": 9.0,
        "empathy": 9.0
    }

    # Low scores should identify improvement areas
    areas_low = llm_analyzer._get_improvement_areas(low_scores)
    assert "ÐŸÑ€Ð¸Ð²ÐµÑ‚ÑÑ‚Ð²Ð¸Ðµ Ð¸ ÐºÐ¾Ð½Ñ‚Ð°ÐºÑ‚" in areas_low
    assert "ÐŸÑ€ÐµÐ·ÐµÐ½Ñ‚Ð°Ñ†Ð¸Ñ Ñ€ÐµÑˆÐµÐ½Ð¸Ñ" in areas_low
    assert "Ð Ð°Ð±Ð¾Ñ‚Ð° Ñ Ð²Ð¾Ð·Ñ€Ð°Ð¶ÐµÐ½Ð¸ÑÐ¼Ð¸" in areas_low
    assert "Ð—Ð°ÐºÑ€Ñ‹Ñ‚Ð¸Ðµ ÑÐ´ÐµÐ»ÐºÐ¸" in areas_low

    # High scores should indicate no improvement needed
    areas_high = llm_analyzer._get_improvement_areas(high_scores)
    assert areas_high == ["Ð’ÑÐµ Ð°ÑÐ¿ÐµÐºÑ‚Ñ‹ Ð´Ð¸Ð°Ð»Ð¾Ð³Ð° Ð² Ð½Ð¾Ñ€Ð¼Ðµ"]


@pytest.mark.asyncio
async def test_analyze_dialog_empty_segments(llm_analyzer):
    """Test analysis with empty segments."""
    with pytest.raises(ValueError, match="No segments provided"):
        await llm_analyzer.analyze_dialog([])


@pytest.mark.asyncio
async def test_analyze_dialog_api_failure(llm_analyzer, sample_segments):
    """Test handling of API failure."""
    # Mock API failure
    llm_analyzer._call_llm_api.side_effect = Exception("API Error")

    with pytest.raises(RuntimeError, match="Analysis failed"):
        await llm_analyzer.analyze_dialog(sample_segments)


@pytest.mark.asyncio
async def test_speaking_time_fallback(llm_analyzer, sample_segments):
    """Test manual speaking time calculation when LLM fails."""
    # Mock LLM failure
    llm_analyzer._call_llm_api.side_effect = Exception("API Error")

    # Override _analyze_speaking_time to test fallback
    manual_result = await llm_analyzer._analyze_speaking_time(sample_segments)

    # Verify manual calculation
    assert manual_result["sales"] > 0
    assert manual_result["customer"] > 0
    assert manual_result["sales"] + manual_result["customer"] == 30.0  # Total duration